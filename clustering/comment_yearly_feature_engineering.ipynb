{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Splitting the comments into yearly datasets then engineering features for each commentor/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned comments and article data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5689832, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load cleaned comment data\n",
    "comments = pd.read_csv('../data/cleaned/comments.csv', header=0, parse_dates=['scrape_datetime','comment_datetime_clean'])\n",
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(202862, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load cleaned article data\n",
    "articles = pd.read_csv('../data/cleaned/articles.csv', header=0, parse_dates=['scrape_datetime','post_datetime'])\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_url</th>\n",
       "      <th>article_body</th>\n",
       "      <th>scrape_datetime</th>\n",
       "      <th>page_url</th>\n",
       "      <th>post_datetime</th>\n",
       "      <th>author</th>\n",
       "      <th>comment_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Week 7 2020 best bets</td>\n",
       "      <td>https://profootballtalk.nbcsports.com/2020/10/...</td>\n",
       "      <td>\\n\\n\\n \\n\\n\\n Getty Images\\n\\nWeek Seven of th...</td>\n",
       "      <td>2020-10-25 10:53:51.516183</td>\n",
       "      <td>https://profootballtalk.nbcsports.com/category...</td>\n",
       "      <td>2020-10-25 10:32:00</td>\n",
       "      <td>Mike Florio</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_title                                        article_url  \\\n",
       "0  Week 7 2020 best bets  https://profootballtalk.nbcsports.com/2020/10/...   \n",
       "\n",
       "                                        article_body  \\\n",
       "0  \\n\\n\\n \\n\\n\\n Getty Images\\n\\nWeek Seven of th...   \n",
       "\n",
       "             scrape_datetime  \\\n",
       "0 2020-10-25 10:53:51.516183   \n",
       "\n",
       "                                            page_url       post_datetime  \\\n",
       "0  https://profootballtalk.nbcsports.com/category... 2020-10-25 10:32:00   \n",
       "\n",
       "        author  comment_counts  \n",
       "0  Mike Florio             0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add article post datetime, text, author name to comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_merge = comments.merge(articles[['article_url','post_datetime','article_body','author']], how='left', on='article_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data by month and engineer features by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Period('2008', 'A-DEC'), Period('2020', 'A-DEC'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the periods that will make up the number of monthly datasets\n",
    "comments_merge['post_datetime_year'] = pd.to_datetime(comments_merge['post_datetime']).dt.to_period('Y')\n",
    "comments_merge.post_datetime_year.min(), comments_merge.post_datetime_year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 \n",
      "\n",
      "2020\n",
      "Wall time: 5.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# How many yearly datasets\n",
    "print(comments_merge.post_datetime_year.nunique(), '\\n')\n",
    "\n",
    "# Iterate through the months to create the datasets\n",
    "for year in comments_merge.post_datetime_year.unique()[:1]:\n",
    "    \n",
    "    # To skip NaT values\n",
    "    if len(str(year)) != 4:\n",
    "        continue\n",
    "    \n",
    "    # Filter data\n",
    "    comments = comments_merge[comments_merge.post_datetime_year == year]\n",
    "    print(year)\n",
    "    \n",
    "    # Calculate the total number of comments by each commentor\n",
    "    commentor_features = pd.DataFrame(comments.groupby(['commentor']).size())\n",
    "    commentor_features['commentor'] = commentor_features.index\n",
    "    commentor_features.columns = ['total_number_of_comments','commentor']\n",
    "    commentor_features.to_csv(f'../data/cluster_features_yearly/01_total_number_of_comments_{year}.csv', header=True, index=True)\n",
    "\n",
    "    # Calculate the number of unique articles the commentor commented on\n",
    "    unique_articles = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_df = pd.DataFrame(unique_articles.groupby(['commentor']).size())\n",
    "    unique_articles_df.columns = ['number_of_articles_commented_on']\n",
    "    unique_articles_df.to_csv(f'../data/cluster_features_yearly/02_unique_articles_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_df.head()\n",
    "    \n",
    "    # Calculate the number of unique articles the commentor commented on exactly once\n",
    "    unique_articles_single_comment = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_single_comment_df = pd.DataFrame(unique_articles_single_comment)\n",
    "    unique_articles_single_comment_df = unique_articles_single_comment_df[unique_articles_single_comment_df[0] == 1]\n",
    "    unique_articles_single_comment_df = pd.DataFrame(unique_articles_single_comment_df.groupby(['commentor']).size())\n",
    "    unique_articles_single_comment_df.columns = ['number_of_articles_w_exactly_one_comment']\n",
    "    unique_articles_single_comment_df.to_csv(f'../data/cluster_features_yearly/03_unique_articles_single_comment_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_single_comment_df.head()\n",
    "    \n",
    "    # Calculate the number of unique articles the commentor commented on more than once\n",
    "    unique_articles_mulitple_comment = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_mulitple_comment_df = pd.DataFrame(unique_articles_mulitple_comment)\n",
    "    unique_articles_mulitple_comment_df = unique_articles_mulitple_comment_df[unique_articles_mulitple_comment_df[0] > 1]\n",
    "    unique_articles_mulitple_comment_df = pd.DataFrame(unique_articles_mulitple_comment_df.groupby(['commentor']).size())\n",
    "    unique_articles_mulitple_comment_df.columns = ['number_of_articles_w_more_than_one_comment']\n",
    "    unique_articles_mulitple_comment_df.to_csv(f'../data/cluster_features_yearly/04_unique_articles_mulitple_comment_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_mulitple_comment_df.head()\n",
    "    \n",
    "    # Calculate how long (in days) a commentor has been active on pft\n",
    "    commentor_activity_duration = comments.groupby(['commentor']).agg({'comment_datetime_clean':['min','max']})\n",
    "    commentor_activity_duration.columns = commentor_activity_duration.columns.droplevel()\n",
    "    commentor_activity_duration['commentor_activity_duration_in_days'] = (commentor_activity_duration['max'] - commentor_activity_duration['min']).dt.days\n",
    "    commentor_activity_duration.to_csv(f'../data/cluster_features_yearly/05_commentor_activity_duration_{year}.csv', header=True, index=True)\n",
    "    # commentor_activity_duration.head()\n",
    "    \n",
    "    # Calcualte the length of the commentor's username\n",
    "    commentor_username_length = comments.groupby(['commentor']).size()\n",
    "    commentor_username_length = pd.DataFrame(commentor_username_length)\n",
    "    commentor_username_length['username'] = commentor_username_length.index\n",
    "    commentor_username_length['username_length'] = commentor_username_length['username'].str.len()\n",
    "    commentor_username_length.drop([0], axis=1, inplace=True)\n",
    "\n",
    "    # Calculate the number of letters, numbers, and spaces in the commentor's username\n",
    "    commentor_username_length['username_alpha_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isalpha() for x in username))\n",
    "    commentor_username_length['username_numeric_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isdigit() for x in username))\n",
    "    commentor_username_length['username_space_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isspace() for x in username))\n",
    "    commentor_username_length.to_csv(f'../data/cluster_features_yearly/06_commentor_username_length_{year}.csv', header=True, index=True)\n",
    "    # commentor_username_length.head()\n",
    "    \n",
    "    # Calculate the mean, median, min and max length of comments (characters)\n",
    "    comments['comment_body_length'] = comments['comment_body'].str.len()\n",
    "    commentor_comment_body_metrics = comments.groupby(['commentor']).agg({'comment_body_length':['mean','median','min','max','sum']})\n",
    "    commentor_comment_body_metrics.columns = commentor_comment_body_metrics.columns.droplevel()\n",
    "    commentor_comment_body_metrics.columns = ['comment_length_mean','comment_length_median','comment_length_min','comment_length_max','comment_length_total']\n",
    "    commentor_comment_body_metrics.to_csv(f'../data/cluster_features_yearly/07_commentor_comment_body_metrics_{year}.csv', header=True, index=True)\n",
    "    # commentor_comment_body_metrics.head()\n",
    "    \n",
    "    # Calculate the average, median, min, max hours between when article was published and comment was made\n",
    "    articles_w_dates = articles.drop_duplicates(subset=['article_url','post_datetime'])\n",
    "    comments_between = pd.merge(comments, articles_w_dates[['article_url','post_datetime']], how='left', on='article_url')\n",
    "    comments_between = comments_between[(comments_between.comment_datetime_clean >= comments_between.post_datetime_x)]\n",
    "    comments_between['hours_btween'] = (comments_between.comment_datetime_clean - comments_between.post_datetime_x) / pd.Timedelta(hours=1)\n",
    "\n",
    "    hours_between_metrics = comments_between.groupby(['commentor']).agg({'hours_btween':['mean','median','min','max']})\n",
    "    hours_between_metrics.columns = hours_between_metrics.columns.droplevel()\n",
    "    hours_between_metrics.columns = ['hours_between_mean','hours_between_median','hours_between_min','hours_between_max']\n",
    "    hours_between_metrics.to_csv(f'../data/cluster_features_yearly/08_hours_between_metrics_{year}.csv', header=True, index=True)\n",
    "    # hours_between_metrics.head()\n",
    "    \n",
    "    # Calculate which days of the week comments were made on\n",
    "    comments['comment_date_dow'] = comments['comment_datetime_clean'].dt.day_name()\n",
    "    comments_dow = pd.pivot_table(comments[['article_url','commentor','comment_date_dow']], index=['commentor'],\n",
    "                        columns=['comment_date_dow'], aggfunc='count', fill_value=0)\n",
    "    comments_dow.columns = comments_dow.columns.droplevel()\n",
    "    comments_dow.columns = ['comments_on_' + c for c in comments_dow.columns]\n",
    "    comments_dow.to_csv(f'../data/cluster_features_yearly/09_comments_dow_{year}.csv', header=True, index=True)\n",
    "    # comments_dow.head()\n",
    "    \n",
    "    # Calculate which hours of the day comments were made on\n",
    "    comments['comment_date_hour'] = comments['comment_datetime_clean'].dt.hour\n",
    "    comments_hour = pd.pivot_table(comments[['article_url','commentor','comment_date_hour']], index=['commentor'],\n",
    "                        columns=['comment_date_hour'], aggfunc='count', fill_value=0)\n",
    "    comments_hour.columns = comments_hour.columns.droplevel()\n",
    "    comments_hour.columns = ['comments_on_hour_' + str(c) for c in comments_hour.columns]\n",
    "    comments_hour.to_csv(f'../data/cluster_features_yearly/10_comments_hour_{year}.csv', header=True, index=True)\n",
    "    # comments_hour.head()\n",
    "    \n",
    "    # Count number of comment made \"in-season\" vs \"off-season\"\n",
    "    # In-season being between 9/1 and 2/1, inclusive\n",
    "    comments['comment_date_month'] = comments['comment_datetime_clean'].dt.month\n",
    "    comments['in_season_flag'] = np.where((comments['comment_date_month'] >= 9) | (comments['comment_date_month'] <= 2), 1, 0)\n",
    "    in_season_comments = comments.groupby(['commentor']).agg({'in_season_flag':['count','sum']})\n",
    "    in_season_comments.columns = in_season_comments.columns.droplevel()\n",
    "    in_season_comments.columns = ['total_comments','number_in_season_comments']\n",
    "    in_season_comments['number_out_season_comments'] = in_season_comments['total_comments'] - in_season_comments['number_in_season_comments']\n",
    "    in_season_comments.to_csv(f'../data/cluster_features_yearly/11_in_season_comments_{year}.csv', header=True, index=True)\n",
    "    # in_season_comments.head()\n",
    "    \n",
    "    # Calculate the most number of comments each commentor posted in a single day\n",
    "    commentor_max_comments = pd.DataFrame(comments.groupby(['commentor','comment_datetime_clean']).size()).reset_index()\n",
    "    commentor_max_comments_df = pd.DataFrame(commentor_max_comments.groupby(['commentor'])[0].max())\n",
    "    commentor_max_comments_df.columns = ['max_number_comments_in_single_day']\n",
    "    commentor_max_comments_df.to_csv(f'../data/cluster_features_yearly/12_commentor_max_comments_df_{year}.csv', header=True, index=True)\n",
    "    # commentor_max_comments_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of comments made by commentors on articles written about each NFL team\n",
    "# Calculate the number of comments made by commentors on articles about each player position (e.g, QB, QB, TE)\n",
    "# Calculate the number of comments made by commentors on articles written by each author\n",
    "# Calculate the number of comments by sentiment: {polarity:[positive, negative, neutral], subjectivity:[0,1]}\n",
    "# Calculate the number of words in the commentor's posts in ALL CAPS\n",
    "# Calculate the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and combine into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01_total_number_of_comments_2008.csv', '01_total_number_of_comments_2009.csv', '01_total_number_of_comments_2010.csv', '01_total_number_of_comments_2011.csv', '01_total_number_of_comments_2012.csv', '01_total_number_of_comments_2013.csv', '01_total_number_of_comments_2014.csv', '01_total_number_of_comments_2015.csv', '01_total_number_of_comments_2016.csv', '01_total_number_of_comments_2017.csv', '01_total_number_of_comments_2018.csv', '01_total_number_of_comments_2019.csv', '01_total_number_of_comments_2020.csv']\n"
     ]
    }
   ],
   "source": [
    "# Inspect files\n",
    "feature_files = os.listdir('../data/cluster_features_yearly/')\n",
    "print(feature_files[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 feature files for year: 2008\n",
      "Found 12 feature files for year: 2009\n",
      "Found 12 feature files for year: 2010\n",
      "Found 12 feature files for year: 2011\n",
      "Found 12 feature files for year: 2012\n",
      "Found 12 feature files for year: 2013\n",
      "Found 12 feature files for year: 2014\n",
      "Found 12 feature files for year: 2015\n",
      "Found 12 feature files for year: 2016\n",
      "Found 12 feature files for year: 2017\n",
      "Found 12 feature files for year: 2018\n",
      "Found 12 feature files for year: 2019\n",
      "Found 12 feature files for year: 2020\n"
     ]
    }
   ],
   "source": [
    "# Validate there are 12 files for all 13 years\n",
    "for year in range(2008,2021):\n",
    "    yearly_files = [f for f in feature_files if str(year) in f]\n",
    "    print(f'Found {len(yearly_files)} feature files for year: {year}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected number of features\n",
    "total_cols = 0\n",
    "for file in yearly_files:\n",
    "    df = pd.read_csv(f'../data/cluster_features_yearly/{file}', nrows=5)\n",
    "    total_cols += df.shape[1] - 1  # Exclude index\n",
    "total_cols    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008 (6854, 57)\n",
      "2009 (26237, 57)\n",
      "2010 (32052, 57)\n",
      "2011 (27624, 57)\n",
      "2012 (34233, 57)\n",
      "2013 (37750, 57)\n",
      "2014 (33157, 57)\n",
      "2015 (31546, 57)\n",
      "2016 (21416, 57)\n",
      "2017 (17742, 57)\n",
      "2018 (16766, 57)\n",
      "2019 (16309, 57)\n",
      "2020 (11074, 57)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the years\n",
    "for year in range(2008,2021):\n",
    "    yearly_files = [f for f in feature_files if str(year) in f]\n",
    "\n",
    "    # Add the path to each feature file\n",
    "    feature_files_w_path = list()\n",
    "    for f in yearly_files:\n",
    "        feature_files_w_path.append(f'../data/cluster_features_yearly/{f}')\n",
    "\n",
    "    # Read the first file to use as seed\n",
    "    df = pd.read_csv(feature_files_w_path[0], header=0, index_col=0)\n",
    "\n",
    "    # Iterate through files and merge into seed\n",
    "    for df_ in feature_files_w_path[1:]:\n",
    "        df_next = pd.read_csv(df_, header=0, index_col=0)\n",
    "        df = df.merge(df_next, on='commentor', how='left')\n",
    "\n",
    "    print(year, df.shape)\n",
    "    \n",
    "    # Write combined dataset to flat file\n",
    "    df.to_csv(f'../data/cleaned/yearly_comment_features/comment_features_{year}.csv', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pft",
   "language": "python",
   "name": "pft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
