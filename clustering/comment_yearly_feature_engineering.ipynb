{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Splitting the comments into yearly datasets then engineering features for each commentor/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned comments and article data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load cleaned comment data\n",
    "comments = pd.read_csv('../data/cleaned/comments.csv', header=0, parse_dates=['scrape_datetime','comment_datetime_clean'])\n",
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load cleaned article data\n",
    "articles = pd.read_csv('../data/cleaned/articles.csv', header=0, parse_dates=['scrape_datetime','post_datetime'])\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add article post datetime, text, author name to comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_merge = comments.merge(articles[['article_url','post_datetime','article_body','author']], how='left', on='article_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data by month and engineer features by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the periods that will make up the number of monthly datasets\n",
    "comments_merge['post_datetime_year'] = pd.to_datetime(comments_merge['post_datetime']).dt.to_period('Y')\n",
    "comments_merge.post_datetime_year.min(), comments_merge.post_datetime_year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# How many yearly datasets\n",
    "print(comments_merge.post_datetime_year.nunique(), '\\n')\n",
    "\n",
    "# Iterate through the months to create the datasets\n",
    "for year in comments_merge.post_datetime_year.unique()[:1]:\n",
    "    \n",
    "    # To skip NaT values\n",
    "    if len(str(year)) != 4:\n",
    "        continue\n",
    "    \n",
    "    # Filter data\n",
    "    comments = comments_merge[comments_merge.post_datetime_year == year]\n",
    "    print(year)\n",
    "    \n",
    "    # Calculate the total number of comments by each commentor\n",
    "    commentor_features = pd.DataFrame(comments.groupby(['commentor']).size())\n",
    "    commentor_features['commentor'] = commentor_features.index\n",
    "    commentor_features.columns = ['total_number_of_comments','commentor']\n",
    "    commentor_features.to_csv(f'../data/cluster_features_yearly/01_total_number_of_comments_{year}.csv', header=True, index=True)\n",
    "\n",
    "    # Calculate the number of unique articles the commentor commented on\n",
    "    unique_articles = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_df = pd.DataFrame(unique_articles.groupby(['commentor']).size())\n",
    "    unique_articles_df.columns = ['number_of_articles_commented_on']\n",
    "    unique_articles_df.to_csv(f'../data/cluster_features_yearly/02_unique_articles_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_df.head()\n",
    "    \n",
    "    # Calculate the number of unique articles the commentor commented on exactly once\n",
    "    unique_articles_single_comment = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_single_comment_df = pd.DataFrame(unique_articles_single_comment)\n",
    "    unique_articles_single_comment_df = unique_articles_single_comment_df[unique_articles_single_comment_df[0] == 1]\n",
    "    unique_articles_single_comment_df = pd.DataFrame(unique_articles_single_comment_df.groupby(['commentor']).size())\n",
    "    unique_articles_single_comment_df.columns = ['number_of_articles_w_exactly_one_comment']\n",
    "    unique_articles_single_comment_df.to_csv(f'../data/cluster_features_yearly/03_unique_articles_single_comment_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_single_comment_df.head()\n",
    "    \n",
    "    # Calculate the number of unique articles the commentor commented on more than once\n",
    "    unique_articles_mulitple_comment = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_mulitple_comment_df = pd.DataFrame(unique_articles_mulitple_comment)\n",
    "    unique_articles_mulitple_comment_df = unique_articles_mulitple_comment_df[unique_articles_mulitple_comment_df[0] > 1]\n",
    "    unique_articles_mulitple_comment_df = pd.DataFrame(unique_articles_mulitple_comment_df.groupby(['commentor']).size())\n",
    "    unique_articles_mulitple_comment_df.columns = ['number_of_articles_w_more_than_one_comment']\n",
    "    unique_articles_mulitple_comment_df.to_csv(f'../data/cluster_features_yearly/04_unique_articles_mulitple_comment_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_mulitple_comment_df.head()\n",
    "    \n",
    "    # Calculate how long (in days) a commentor has been active on pft\n",
    "    commentor_activity_duration = comments.groupby(['commentor']).agg({'comment_datetime_clean':['min','max']})\n",
    "    commentor_activity_duration.columns = commentor_activity_duration.columns.droplevel()\n",
    "    commentor_activity_duration['commentor_activity_duration_in_days'] = (commentor_activity_duration['max'] - commentor_activity_duration['min']).dt.days\n",
    "    commentor_activity_duration.to_csv(f'../data/cluster_features_yearly/05_commentor_activity_duration_{year}.csv', header=True, index=True)\n",
    "    # commentor_activity_duration.head()\n",
    "    \n",
    "    # Calcualte the length of the commentor's username\n",
    "    commentor_username_length = comments.groupby(['commentor']).size()\n",
    "    commentor_username_length = pd.DataFrame(commentor_username_length)\n",
    "    commentor_username_length['username'] = commentor_username_length.index\n",
    "    commentor_username_length['username_length'] = commentor_username_length['username'].str.len()\n",
    "    commentor_username_length.drop([0], axis=1, inplace=True)\n",
    "\n",
    "    # Calculate the number of letters, numbers, and spaces in the commentor's username\n",
    "    commentor_username_length['username_alpha_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isalpha() for x in username))\n",
    "    commentor_username_length['username_numeric_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isdigit() for x in username))\n",
    "    commentor_username_length['username_space_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isspace() for x in username))\n",
    "    commentor_username_length.to_csv(f'../data/cluster_features_yearly/06_commentor_username_length_{year}.csv', header=True, index=True)\n",
    "    # commentor_username_length.head()\n",
    "    \n",
    "    # Calculate the mean, median, min and max length of comments (characters)\n",
    "    comments['comment_body_length'] = comments['comment_body'].str.len()\n",
    "    commentor_comment_body_metrics = comments.groupby(['commentor']).agg({'comment_body_length':['mean','median','min','max','sum']})\n",
    "    commentor_comment_body_metrics.columns = commentor_comment_body_metrics.columns.droplevel()\n",
    "    commentor_comment_body_metrics.columns = ['comment_length_mean','comment_length_median','comment_length_min','comment_length_max','comment_length_total']\n",
    "    commentor_comment_body_metrics.to_csv(f'../data/cluster_features_yearly/07_commentor_comment_body_metrics_{year}.csv', header=True, index=True)\n",
    "    # commentor_comment_body_metrics.head()\n",
    "    \n",
    "    # Calculate the average, median, min, max hours between when article was published and comment was made\n",
    "    articles_w_dates = articles.drop_duplicates(subset=['article_url','post_datetime'])\n",
    "    comments_between = pd.merge(comments, articles_w_dates[['article_url','post_datetime']], how='left', on='article_url')\n",
    "    comments_between = comments_between[(comments_between.comment_datetime_clean >= comments_between.post_datetime_x)]\n",
    "    comments_between['hours_btween'] = (comments_between.comment_datetime_clean - comments_between.post_datetime_x) / pd.Timedelta(hours=1)\n",
    "\n",
    "    hours_between_metrics = comments_between.groupby(['commentor']).agg({'hours_btween':['mean','median','min','max']})\n",
    "    hours_between_metrics.columns = hours_between_metrics.columns.droplevel()\n",
    "    hours_between_metrics.columns = ['hours_between_mean','hours_between_median','hours_between_min','hours_between_max']\n",
    "    hours_between_metrics.to_csv(f'../data/cluster_features_yearly/08_hours_between_metrics_{year}.csv', header=True, index=True)\n",
    "    # hours_between_metrics.head()\n",
    "    \n",
    "    # Calculate which days of the week comments were made on\n",
    "    comments['comment_date_dow'] = comments['comment_datetime_clean'].dt.day_name()\n",
    "    comments_dow = pd.pivot_table(comments[['article_url','commentor','comment_date_dow']], index=['commentor'],\n",
    "                        columns=['comment_date_dow'], aggfunc='count', fill_value=0)\n",
    "    comments_dow.columns = comments_dow.columns.droplevel()\n",
    "    comments_dow.columns = ['comments_on_' + c for c in comments_dow.columns]\n",
    "    comments_dow.to_csv(f'../data/cluster_features_yearly/09_comments_dow_{year}.csv', header=True, index=True)\n",
    "    # comments_dow.head()\n",
    "    \n",
    "    # Calculate which hours of the day comments were made on\n",
    "    comments['comment_date_hour'] = comments['comment_datetime_clean'].dt.hour\n",
    "    comments_hour = pd.pivot_table(comments[['article_url','commentor','comment_date_hour']], index=['commentor'],\n",
    "                        columns=['comment_date_hour'], aggfunc='count', fill_value=0)\n",
    "    comments_hour.columns = comments_hour.columns.droplevel()\n",
    "    comments_hour.columns = ['comments_on_hour_' + str(c) for c in comments_hour.columns]\n",
    "    comments_hour.to_csv(f'../data/cluster_features_yearly/10_comments_hour_{year}.csv', header=True, index=True)\n",
    "    # comments_hour.head()\n",
    "    \n",
    "    # Count number of comment made \"in-season\" vs \"off-season\"\n",
    "    # In-season being between 9/1 and 2/1, inclusive\n",
    "    comments['comment_date_month'] = comments['comment_datetime_clean'].dt.month\n",
    "    comments['in_season_flag'] = np.where((comments['comment_date_month'] >= 9) | (comments['comment_date_month'] <= 2), 1, 0)\n",
    "    in_season_comments = comments.groupby(['commentor']).agg({'in_season_flag':['count','sum']})\n",
    "    in_season_comments.columns = in_season_comments.columns.droplevel()\n",
    "    in_season_comments.columns = ['total_comments','number_in_season_comments']\n",
    "    in_season_comments['number_out_season_comments'] = in_season_comments['total_comments'] - in_season_comments['number_in_season_comments']\n",
    "    in_season_comments.to_csv(f'../data/cluster_features_yearly/11_in_season_comments_{year}.csv', header=True, index=True)\n",
    "    # in_season_comments.head()\n",
    "    \n",
    "    # Calculate the most number of comments each commentor posted in a single day\n",
    "    commentor_max_comments = pd.DataFrame(comments.groupby(['commentor','comment_datetime_clean']).size()).reset_index()\n",
    "    commentor_max_comments_df = pd.DataFrame(commentor_max_comments.groupby(['commentor'])[0].max())\n",
    "    commentor_max_comments_df.columns = ['max_number_comments_in_single_day']\n",
    "    commentor_max_comments_df.to_csv(f'../data/cluster_features_yearly/12_commentor_max_comments_df_{year}.csv', header=True, index=True)\n",
    "    # commentor_max_comments_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate the number of comments made by commentors on articles written about each NFL team\n",
    "\n",
    "# Read in nfl teams\n",
    "with open('../data/reference/nfl_team_names.txt') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# Create DataFrame of unique articles (added for efficiency)\n",
    "articles_yearly = pd.DataFrame(comments['article_body'])\n",
    "articles_yearly_unique = articles_yearly.drop_duplicates()\n",
    "\n",
    "# Flag articles by which teams they mention\n",
    "nfl_teams = [x.strip() for x in content]\n",
    "for team in nfl_teams:\n",
    "    articles_yearly_unique[f'article_mentions_{team}'] = articles_yearly_unique['article_body'].str.lower().str.contains(f'{team}')\n",
    "\n",
    "# Combine comments with articles flagged by team and fix redskins/WFT name change\n",
    "comments_with_teams = pd.merge(comments, articles_yearly_unique, how='left', on='article_body')\n",
    "comments_with_teams['article_mentions_football_team'] = comments_with_teams['article_mentions_football team'] + comments_with_teams['article_mentions_redskins']\n",
    "comments_with_teams.drop(labels=['article_mentions_redskins','article_mentions_football team'], axis=1, inplace=True)\n",
    "\n",
    "# Aggregate number of articles commented on by \n",
    "cols_to_sum = [c for c in comments_with_teams if 'article_mentions_' in c]\n",
    "commentor_by_articles_mentioning_team = comments_with_teams.groupby(['commentor'])[cols_to_sum].apply(lambda x : x.astype(int).sum())\n",
    "commentor_by_articles_mentioning_team.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate the number of comments made by commentors on articles about each player position (e.g, QB, QB, TE)\n",
    "player_positions_full = ['quarterback','wide receiver','tackle','guard','center','tight end','fullback','halfback','running back',\n",
    "                    'cornerback','linebacker','defensive end','safety', 'kicker']\n",
    "\n",
    "# https://help.yahoo.com/kb/position-abbreviations-eligibility-players-play-positions-sln6500.html\n",
    "# player_positions_abbreviation = ['QB','WR','OL','DL','TE','FB','RB','CB','LB','DE','S','ST']   # can we include position abbreivations that are only one char: S, K, T\n",
    "# positions = player_positions_full + player_positions_abbreviation\n",
    "\n",
    "# Create DataFrame of unique articles (added for efficiency)\n",
    "articles_yearly = pd.DataFrame(comments['article_body'])\n",
    "articles_yearly_unique = articles_yearly.drop_duplicates()\n",
    "\n",
    "for position in player_positions_full:\n",
    "    articles_yearly_unique[f'article_mentions_{position}'] = articles_yearly_unique['article_body'].str.lower().str.contains(f'{position}')\n",
    "\n",
    "# Combine comments with articles flagged by team and fix redskins/WFT name change\n",
    "comments_with_positions = pd.merge(comments, articles_yearly_unique, how='left', on='article_body')\n",
    "\n",
    "# Aggregate number of articles commented on by \n",
    "cols_to_sum = [c for c in comments_with_positions if 'article_mentions_' in c]\n",
    "commentor_by_articles_mentioning_position = comments_with_positions.groupby(['commentor'])[cols_to_sum].apply(lambda x : x.astype(int).sum())\n",
    "commentor_by_articles_mentioning_position.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of comments made by commentors on articles about \"offense\" vs. \"defense\"\n",
    "comments['article_mentions_offense'] = comments['article_body'].str.lower().str.contains('offense')\n",
    "comments['article_mentions_offense'] = comments['article_body'].str.lower().str.contains('defense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of comments made by commentors on articles that mention \"fantasy\"\n",
    "comments['article_mentions_offense'] = comments['article_body'].str.lower().str.contains('fantasy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of comments made by commentors on articles that mention \"injured\",\"injury\", \"injuries\",\"questionable\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of comments made by commentors on articles that mention \"contract\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of comments made by commentors on articles about \"offense\" vs. \"defense\"\n",
    "# Calculate the number of comments made by commentors on articles that mention \"fantasy\"\n",
    "# Calculate the number of comments made by commentors on articles that mention \"injured\",\"injury\", \"injuries\",\"questionable\"\n",
    "# Calculate the number of comments made by commentors on articles that mention \"contract\"\n",
    "\n",
    "# Calculate the number of comments made by commentors on articles written by each author\n",
    "# Calculate the number of comments by sentiment: {polarity:[positive, negative, neutral], subjectivity:[0,1]}\n",
    "# Calculate the number of words in the commentor's posts in ALL CAPS\n",
    "# Calculate readability metrics -> https://github.com/cdimascio/py-readability-metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and combine into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect files\n",
    "feature_files = os.listdir('../data/cluster_features_yearly/')\n",
    "print(feature_files[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate there are 12 files for all 13 years\n",
    "for year in range(2008,2021):\n",
    "    yearly_files = [f for f in feature_files if str(year) in f]\n",
    "    print(f'Found {len(yearly_files)} feature files for year: {year}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected number of features\n",
    "total_cols = 0\n",
    "for file in yearly_files:\n",
    "    df = pd.read_csv(f'../data/cluster_features_yearly/{file}', nrows=5)\n",
    "    total_cols += df.shape[1] - 1  # Exclude index\n",
    "total_cols    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the years\n",
    "for year in range(2008,2021):\n",
    "    yearly_files = [f for f in feature_files if str(year) in f]\n",
    "\n",
    "    # Add the path to each feature file\n",
    "    feature_files_w_path = list()\n",
    "    for f in yearly_files:\n",
    "        feature_files_w_path.append(f'../data/cluster_features_yearly/{f}')\n",
    "\n",
    "    # Read the first file to use as seed\n",
    "    df = pd.read_csv(feature_files_w_path[0], header=0, index_col=0)\n",
    "\n",
    "    # Iterate through files and merge into seed\n",
    "    for df_ in feature_files_w_path[1:]:\n",
    "        df_next = pd.read_csv(df_, header=0, index_col=0)\n",
    "        df = df.merge(df_next, on='commentor', how='left')\n",
    "\n",
    "    print(year, df.shape)\n",
    "    \n",
    "    # Write combined dataset to flat file\n",
    "    df.to_csv(f'../data/cleaned/yearly_comment_features/comment_features_{year}.csv', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pft",
   "language": "python",
   "name": "pft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
