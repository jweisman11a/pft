{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Splitting the comments into monthly datasets then engineering features for each commentor/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned comments and article data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5689832, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load cleaned comment data\n",
    "comments = pd.read_csv('../data/cleaned/comments.csv', header=0, parse_dates=['scrape_datetime','comment_datetime_clean'])\n",
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(202862, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load cleaned article data\n",
    "articles = pd.read_csv('../data/cleaned/articles.csv', header=0, parse_dates=['scrape_datetime','post_datetime'])\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add article post datetime to comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_merge = comments.merge(articles[['article_url','post_datetime']], how='left', on='article_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data by month and engineer features by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Period('2008', 'A-DEC'), Period('2020', 'A-DEC'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the periods that will make up the number of monthly datasets\n",
    "comments_merge['post_datetime_year'] = pd.to_datetime(comments_merge['post_datetime']).dt.to_period('Y')\n",
    "comments_merge.post_datetime_year.min(), comments_merge.post_datetime_year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 \n",
      "\n",
      "2020\n",
      "2019\n",
      "2018\n",
      "2017\n",
      "2016\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2012\n",
      "2011\n",
      "2010\n",
      "2009\n",
      "2008\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# How many yearly datasets\n",
    "print(comments_merge.post_datetime_year.nunique(), '\\n')\n",
    "\n",
    "# Iterate through the months to create the datasets\n",
    "for year in comments_merge.post_datetime_year.unique():\n",
    "    \n",
    "    # To skip NaT values\n",
    "    if len(str(year)) != 4:\n",
    "        continue\n",
    "    \n",
    "    # Filter data\n",
    "    comments = comments_merge[comments_merge.post_datetime_year == year]\n",
    "    print(year)\n",
    "    \n",
    "    # Calculate the total number of comments by each commentor\n",
    "    commentor_features = pd.DataFrame(comments.groupby(['commentor']).size())\n",
    "    commentor_features['commentor'] = commentor_features.index\n",
    "    commentor_features.columns = ['total_number_of_comments','commentor']\n",
    "    commentor_features.to_csv(f'../data/cluster_features_monthly/01_total_number_of_comments_{year}.csv', header=True, index=True)\n",
    "\n",
    "    # Calculate the number of unique articles the commentor commented on\n",
    "    unique_articles = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_df = pd.DataFrame(unique_articles.groupby(['commentor']).size())\n",
    "    unique_articles_df.columns = ['number_of_articles_commented_on']\n",
    "    unique_articles_df.to_csv(f'../data/cluster_features_monthly/02_unique_articles_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_df.head()\n",
    "    \n",
    "    # Calculate the number of unique articles the commentor commented on exactly once\n",
    "    unique_articles_single_comment = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_single_comment_df = pd.DataFrame(unique_articles_single_comment)\n",
    "    unique_articles_single_comment_df = unique_articles_single_comment_df[unique_articles_single_comment_df[0] == 1]\n",
    "    unique_articles_single_comment_df = pd.DataFrame(unique_articles_single_comment_df.groupby(['commentor']).size())\n",
    "    unique_articles_single_comment_df.columns = ['number_of_articles_w_exactly_one_comment']\n",
    "    unique_articles_single_comment_df.to_csv(f'../data/cluster_features_monthly/03_unique_articles_single_comment_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_single_comment_df.head()\n",
    "    \n",
    "    # Calculate the number of unique articles the commentor commented on more than once\n",
    "    unique_articles_mulitple_comment = comments.groupby(['commentor','article_url']).size()\n",
    "    unique_articles_mulitple_comment_df = pd.DataFrame(unique_articles_mulitple_comment)\n",
    "    unique_articles_mulitple_comment_df = unique_articles_mulitple_comment_df[unique_articles_mulitple_comment_df[0] > 1]\n",
    "    unique_articles_mulitple_comment_df = pd.DataFrame(unique_articles_mulitple_comment_df.groupby(['commentor']).size())\n",
    "    unique_articles_mulitple_comment_df.columns = ['number_of_articles_w_more_than_one_comment']\n",
    "    unique_articles_mulitple_comment_df.to_csv(f'../data/cluster_features_monthly/04_unique_articles_mulitple_comment_df_{year}.csv', header=True, index=True)\n",
    "    # unique_articles_mulitple_comment_df.head()\n",
    "    \n",
    "    # Calculate how long (in days) a commentor has been active on pft\n",
    "    commentor_activity_duration = comments.groupby(['commentor']).agg({'comment_datetime_clean':['min','max']})\n",
    "    commentor_activity_duration.columns = commentor_activity_duration.columns.droplevel()\n",
    "    commentor_activity_duration['commentor_activity_duration_in_days'] = (commentor_activity_duration['max'] - commentor_activity_duration['min']).dt.days\n",
    "    commentor_activity_duration.to_csv(f'../data/cluster_features_monthly/05_commentor_activity_duration_{year}.csv', header=True, index=True)\n",
    "    # commentor_activity_duration.head()\n",
    "    \n",
    "    # Calcualte the length of the commentor's username\n",
    "    commentor_username_length = comments.groupby(['commentor']).size()\n",
    "    commentor_username_length = pd.DataFrame(commentor_username_length)\n",
    "    commentor_username_length['username'] = commentor_username_length.index\n",
    "    commentor_username_length['username_length'] = commentor_username_length['username'].str.len()\n",
    "    commentor_username_length.drop([0], axis=1, inplace=True)\n",
    "\n",
    "    # Calculate the number of letters, numbers, and spaces in the commentor's username\n",
    "    commentor_username_length['username_alpha_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isalpha() for x in username))\n",
    "    commentor_username_length['username_numeric_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isdigit() for x in username))\n",
    "    commentor_username_length['username_space_chars'] = commentor_username_length['username'].apply(lambda username: sum(x.isspace() for x in username))\n",
    "    commentor_username_length.to_csv(f'../data/cluster_features_monthly/06_commentor_username_length_{year}.csv', header=True, index=True)\n",
    "    # commentor_username_length.head()\n",
    "    \n",
    "    # Calculate the mean, median, min and max length of comments (characters)\n",
    "    comments['comment_body_length'] = comments['comment_body'].str.len()\n",
    "    commentor_comment_body_metrics = comments.groupby(['commentor']).agg({'comment_body_length':['mean','median','min','max','sum']})\n",
    "    commentor_comment_body_metrics.columns = commentor_comment_body_metrics.columns.droplevel()\n",
    "    commentor_comment_body_metrics.columns = ['comment_length_mean','comment_length_median','comment_length_min','comment_length_max','comment_length_total']\n",
    "    commentor_comment_body_metrics.to_csv(f'../data/cluster_features_monthly/07_commentor_comment_body_metrics_{year}.csv', header=True, index=True)\n",
    "    # commentor_comment_body_metrics.head()\n",
    "    \n",
    "    # Calculate the average, median, min, max hours between when article was published and comment was made\n",
    "    articles_w_dates = articles.drop_duplicates(subset=['article_url','post_datetime'])\n",
    "    comments_between = pd.merge(comments, articles_w_dates[['article_url','post_datetime']], how='left', on='article_url')\n",
    "    comments_between = comments_between[(comments_between.comment_datetime_clean >= comments_between.post_datetime_x)]\n",
    "    comments_between['hours_btween'] = (comments_between.comment_datetime_clean - comments_between.post_datetime_x) / pd.Timedelta(hours=1)\n",
    "\n",
    "    hours_between_metrics = comments_between.groupby(['commentor']).agg({'hours_btween':['mean','median','min','max']})\n",
    "    hours_between_metrics.columns = hours_between_metrics.columns.droplevel()\n",
    "    hours_between_metrics.columns = ['hours_between_mean','hours_between_median','hours_between_min','hours_between_max']\n",
    "    hours_between_metrics.to_csv(f'../data/cluster_features_monthly/08_hours_between_metrics_{year}.csv', header=True, index=True)\n",
    "    # hours_between_metrics.head()\n",
    "    \n",
    "    # Calculate which days of the week comments were made on\n",
    "    comments['comment_date_dow'] = comments['comment_datetime_clean'].dt.day_name()\n",
    "    comments_dow = pd.pivot_table(comments[['article_url','commentor','comment_date_dow']], index=['commentor'],\n",
    "                        columns=['comment_date_dow'], aggfunc='count', fill_value=0)\n",
    "    comments_dow.columns = comments_dow.columns.droplevel()\n",
    "    comments_dow.columns = ['comments_on_' + c for c in comments_dow.columns]\n",
    "    comments_dow.to_csv(f'../data/cluster_features_monthly/09_comments_dow_{year}.csv', header=True, index=True)\n",
    "    # comments_dow.head()\n",
    "    \n",
    "    # Calculate which hours of the day comments were made on\n",
    "    comments['comment_date_hour'] = comments['comment_datetime_clean'].dt.hour\n",
    "    comments_hour = pd.pivot_table(comments[['article_url','commentor','comment_date_hour']], index=['commentor'],\n",
    "                        columns=['comment_date_hour'], aggfunc='count', fill_value=0)\n",
    "    comments_hour.columns = comments_hour.columns.droplevel()\n",
    "    comments_hour.columns = ['comments_on_hour_' + str(c) for c in comments_hour.columns]\n",
    "    comments_hour.to_csv(f'../data/cluster_features_monthly/10_comments_hour_{year}.csv', header=True, index=True)\n",
    "    # comments_hour.head()\n",
    "    \n",
    "    # Count number of comment made \"in-season\" vs \"off-season\"\n",
    "    # In-season being between 9/1 and 2/1, inclusive\n",
    "    comments['comment_date_month'] = comments['comment_datetime_clean'].dt.month\n",
    "    comments['in_season_flag'] = np.where((comments['comment_date_month'] >= 9) | (comments['comment_date_month'] <= 2), 1, 0)\n",
    "    in_season_comments = comments.groupby(['commentor']).agg({'in_season_flag':['count','sum']})\n",
    "    in_season_comments.columns = in_season_comments.columns.droplevel()\n",
    "    in_season_comments.columns = ['total_comments','number_in_season_comments']\n",
    "    in_season_comments['number_out_season_comments'] = in_season_comments['total_comments'] - in_season_comments['number_in_season_comments']\n",
    "    in_season_comments.to_csv(f'../data/cluster_features_monthly/11_in_season_comments_{year}.csv', header=True, index=True)\n",
    "    # in_season_comments.head()\n",
    "    \n",
    "    # Calculate the most number of comments each commentor posted in a single day\n",
    "    commentor_max_comments = pd.DataFrame(comments.groupby(['commentor','comment_datetime_clean']).size()).reset_index()\n",
    "    commentor_max_comments_df = pd.DataFrame(commentor_max_comments.groupby(['commentor'])[0].max())\n",
    "    commentor_max_comments_df.columns = ['max_number_comments_in_single_day']\n",
    "    commentor_max_comments_df.to_csv(f'../data/cluster_features_monthly/12_commentor_max_comments_df_{year}.csv', header=True, index=True)\n",
    "    # commentor_max_comments_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and combine into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01_total_number_of_comments_2008.csv', '01_total_number_of_comments_2009.csv', '01_total_number_of_comments_2010.csv', '01_total_number_of_comments_2011.csv', '01_total_number_of_comments_2012.csv', '01_total_number_of_comments_2013.csv', '01_total_number_of_comments_2014.csv', '01_total_number_of_comments_2015.csv', '01_total_number_of_comments_2016.csv', '01_total_number_of_comments_2017.csv', '01_total_number_of_comments_2018.csv', '01_total_number_of_comments_2019.csv', '01_total_number_of_comments_2020.csv']\n"
     ]
    }
   ],
   "source": [
    "# Inspect files\n",
    "feature_files = os.listdir('../data/cluster_features_monthly/')\n",
    "print(feature_files[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 feature files for year: 2008\n",
      "Found 12 feature files for year: 2009\n",
      "Found 12 feature files for year: 2010\n",
      "Found 12 feature files for year: 2011\n",
      "Found 12 feature files for year: 2012\n",
      "Found 12 feature files for year: 2013\n",
      "Found 12 feature files for year: 2014\n",
      "Found 12 feature files for year: 2015\n",
      "Found 12 feature files for year: 2016\n",
      "Found 12 feature files for year: 2017\n",
      "Found 12 feature files for year: 2018\n",
      "Found 12 feature files for year: 2019\n",
      "Found 12 feature files for year: 2020\n"
     ]
    }
   ],
   "source": [
    "# Validate there are 12 files for all 13 years\n",
    "for year in range(2008,2021):\n",
    "    yearly_files = [f for f in feature_files if str(year) in f]\n",
    "    print(f'Found {len(yearly_files)} feature files for year: {year}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected number of features\n",
    "total_cols = 0\n",
    "for file in yearly_files:\n",
    "    df = pd.read_csv(f'../data/cluster_features_monthly/{file}', nrows=5)\n",
    "    total_cols += df.shape[1] - 1  # Exclude index\n",
    "total_cols    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008 (6854, 57)\n",
      "2009 (26237, 57)\n",
      "2010 (32052, 57)\n",
      "2011 (27624, 57)\n",
      "2012 (34233, 57)\n",
      "2013 (37750, 57)\n",
      "2014 (33157, 57)\n",
      "2015 (31546, 57)\n",
      "2016 (21416, 57)\n",
      "2017 (17742, 57)\n",
      "2018 (16766, 57)\n",
      "2019 (16309, 57)\n",
      "2020 (11074, 57)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the years\n",
    "for year in range(2008,2021):\n",
    "    yearly_files = [f for f in feature_files if str(year) in f]\n",
    "\n",
    "    # Add the path to each feature file\n",
    "    feature_files_w_path = list()\n",
    "    for f in yearly_files:\n",
    "        feature_files_w_path.append(f'../data/cluster_features_monthly/{f}')\n",
    "\n",
    "    # Read the first file to use as seed\n",
    "    df = pd.read_csv(feature_files_w_path[0], header=0, index_col=0)\n",
    "\n",
    "    # Iterate through files and merge into seed\n",
    "    for df_ in feature_files_w_path[1:]:\n",
    "        df_next = pd.read_csv(df_, header=0, index_col=0)\n",
    "        df = df.merge(df_next, on='commentor', how='left')\n",
    "\n",
    "    print(year, df.shape)\n",
    "    \n",
    "    # Write combined dataset to flat file\n",
    "    df.to_csv(f'../data/cleaned/yearly_comment_features/comment_features_{year}.csv', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pft",
   "language": "python",
   "name": "pft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
